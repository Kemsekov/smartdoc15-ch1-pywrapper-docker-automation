{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smartdoc15_ch1 import Dataset, evaluate_segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = Dataset(data_home=\"/data/competitions/2015-ICDAR-smartdoc/challenge1/99-computable-version-2017-test\",\n",
    "           download_if_missing=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 8)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seg_gt = d.segmentation_targets\n",
    "seg_gt.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation ground truth vs ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------\n",
      "   evaluate_segmentation: Evaluation report\n",
      "----------------------------------------------\n",
      "metric: IoU (aka Jaccard index)\n",
      "----------------------------------------------\n",
      "observations:    50\n",
      "mean:             1.00 (CI@95%: 1.000, 1.000)\n",
      "min-max:          1.000 - 1.000\n",
      "variance:         0.000 (std: 0.000)\n",
      "----------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_segmentation(seg_gt, seg_gt, d.model_shapes, print_summary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation ground truth scaled vs ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_reduced = d.segmentation_targets * 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------\n",
      "   evaluate_segmentation: Evaluation report\n",
      "----------------------------------------------\n",
      "metric: IoU (aka Jaccard index)\n",
      "----------------------------------------------\n",
      "observations:    50\n",
      "mean:             1.00 (CI@95%: 1.000, 1.000)\n",
      "min-max:          1.000 - 1.000\n",
      "variance:         0.000 (std: 0.000)\n",
      "----------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_segmentation(seg_reduced, seg_gt, d.model_shapes, frame_scale_factor=0.5, print_summary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output contains the individual evaluation for each observation, making is easy to merge evaluations from different subsets, perform cross validation, identify problematic elements, or simply to extract more elaborate statistics about those values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.tile?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "observ = seg_gt.shape[0]\n",
    "framew, frameh = 1920., 1080."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 8)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wholeframeseg = np.array([0., 0., 0., frameh, framew, frameh, framew, 0.])\n",
    "fakeresults1 = np.tile(wholeframeseg, (observ, 1))\n",
    "fakeresults1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------\n",
      "   evaluate_segmentation: Evaluation report\n",
      "----------------------------------------------\n",
      "metric: IoU (aka Jaccard index)\n",
      "----------------------------------------------\n",
      "observations:    50\n",
      "mean:             0.11 (CI@95%: 0.100, 0.119)\n",
      "min-max:          0.041 - 0.173\n",
      "variance:         0.001 (std: 0.034)\n",
      "----------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.13641666,  0.07597955,  0.07346789,  0.14002696,  0.06881893,\n",
       "        0.09835207,  0.12968018,  0.08838395,  0.06561334,  0.13469754,\n",
       "        0.12902699,  0.14044994,  0.09351552,  0.05851509,  0.08977727,\n",
       "        0.11079546,  0.0817408 ,  0.17336802,  0.11894965,  0.13006599,\n",
       "        0.13305495,  0.11265368,  0.16414012,  0.0642706 ,  0.13675877,\n",
       "        0.1159726 ,  0.1670184 ,  0.11575269,  0.15434624,  0.14086942,\n",
       "        0.14107757,  0.08068005,  0.08205667,  0.08849093,  0.14463279,\n",
       "        0.0752603 ,  0.15033108,  0.13632017,  0.11354156,  0.0778586 ,\n",
       "        0.08139904,  0.04117838,  0.14629553,  0.10258302,  0.07059613,\n",
       "        0.14151006,  0.15199741,  0.07204286,  0.06628012,  0.05887742])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_segmentation(fakeresults1, seg_gt, d.model_shapes, print_summary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 8)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fakeresults2 = np.tile(seg_gt.mean(axis=0), (observ, 1))\n",
    "fakeresults2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------\n",
      "   evaluate_segmentation: Evaluation report\n",
      "----------------------------------------------\n",
      "metric: IoU (aka Jaccard index)\n",
      "----------------------------------------------\n",
      "observations:    50\n",
      "mean:             0.46 (CI@95%: 0.423, 0.507)\n",
      "min-max:          0.189 - 0.805\n",
      "variance:         0.023 (std: 0.150)\n",
      "----------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.45183008,  0.55355323,  0.38872883,  0.36433957,  0.78977809,\n",
       "        0.2646679 ,  0.36152687,  0.18923517,  0.54230772,  0.45843342,\n",
       "        0.37718844,  0.43986082,  0.42138467,  0.67858408,  0.66141109,\n",
       "        0.3610824 ,  0.47823445,  0.353207  ,  0.37212445,  0.34927205,\n",
       "        0.46757727,  0.49855925,  0.26525091,  0.75225125,  0.43816964,\n",
       "        0.52902661,  0.36518089,  0.47279823,  0.37296018,  0.44090575,\n",
       "        0.44430899,  0.64669361,  0.40195558,  0.2053788 ,  0.28261313,\n",
       "        0.80487323,  0.35685596,  0.24277402,  0.46944937,  0.72896937,\n",
       "        0.75070575,  0.36296816,  0.4128719 ,  0.56657698,  0.55004232,\n",
       "        0.43794655,  0.40503238,  0.60140644,  0.47140204,  0.64772461])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_segmentation(fakeresults2, seg_gt, d.model_shapes, print_summary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seg_list_of_dict_to_array(list_of_dict):\n",
    "    return np.float32([\n",
    "        [e[k] \n",
    "        for k in ('tl_x', 'tl_y', 'bl_x', 'bl_y', 'br_x', 'br_y', 'tr_x', 'tr_y')]\n",
    "        for e in list_of_dict])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_listdict_1 = [\n",
    "    {'tl_x': 0., 'tl_y': 0., 'bl_x': 0., 'bl_y': frameh, 'br_x': framew, 'br_y':frameh, 'tr_x': framew, 'tr_y': 0.},\n",
    "    {'tl_x': 0., 'tl_y': 0., 'bl_x': 0., 'bl_y': frameh, 'br_x': framew, 'br_y':frameh, 'tr_x': framew, 'tr_y': 0.},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_aa_1 = seg_list_of_dict_to_array(seg_listdict_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0.,     0.,     0.,  1080.,  1920.,  1080.,  1920.,     0.],\n",
       "       [    0.,     0.,     0.,  1080.,  1920.,  1080.,  1920.,     0.]], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seg_aa_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------\n",
      "   evaluate_segmentation: Evaluation report\n",
      "----------------------------------------------\n",
      "metric: IoU (aka Jaccard index)\n",
      "----------------------------------------------\n",
      "observations:     2\n",
      "mean:             0.11 (CI@95%: 0.047, 0.165)\n",
      "min-max:          0.076 - 0.136\n",
      "variance:         0.002 (std: 0.043)\n",
      "----------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.13641666,  0.07597955])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_segmentation(seg_aa_1, seg_gt[:2], d.model_shapes[:2], print_summary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
